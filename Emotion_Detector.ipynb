{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Emotion_Detector.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yrsltn/Integrated-Circuit-Desing-in-Verilog/blob/master/Emotion_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNFpoFJs5Q71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3402ab4-c2f3-4948-e90a-7f226baa68f2"
      },
      "source": [
        "!pip3 install ktrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ktrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/52/aa566f14064dd8904e37d2346431465f20a053df8ad89eae246dcc073dc5/ktrain-0.25.0.tar.gz (25.3MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3MB 137kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.1.4)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.0)\n",
            "Collecting keras_bert>=0.86.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.17.0)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/e5/a0b9edd8664ea3b0d3270c451ebbf86655ed9fc4c3e4c45b9afae9c2e382/cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl (263kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.5)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.1)\n",
            "Collecting seqeval==0.0.19\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e5/b7705156a77f742cfe4fc6f22d0c71591edb2d243328dff2f8fc0f933ab6/seqeval-0.0.19.tar.gz\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.4)\n",
            "Collecting transformers>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n",
            "Collecting syntok\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/76/a49e73a04b3e3a14ce232e8e28a1587f8108baa665644fe8c40e307e792e/syntok-1.3.1.tar.gz\n",
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ktrain) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ktrain) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras_bert>=0.86.0->ktrain) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->ktrain) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.3->ktrain) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (7.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.11.2)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (0.8)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (50.3.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.6.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert>=0.86.0->ktrain) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.1.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: ktrain, keras-bert, langdetect, seqeval, syntok, keras-transformer, sacremoses, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.25.0-cp36-none-any.whl size=25274500 sha256=ff2067caf43ad3833c257d5b737ad602217aaca310aea4e7b13512b434b44c9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/73/05/f36d0027bb6575384e21506dbba8db36a7825f15a24f09b2d5\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=336e9aa686fa0b61ac42977de0a334aafe2759fce43269b9c6258659df94ece8\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=a988d585a9b77dab49a8bea027ec5af5e8bcea5c4886b4cdeda8d94a049f2bdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.19-cp36-none-any.whl size=9919 sha256=0dd08ae9b71909321825ab3007a1d044245250fbef9d58a43a53c2879e7141c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/bf/1198beceed805a2099060975f6281d1b01046dd279e19c97be\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syntok: filename=syntok-1.3.1-cp36-none-any.whl size=20919 sha256=7f91382b0357d1dad82c5cb830ef50d65c31af6f7b10904bff8048719c48ca16\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/c6/a4/be1920586c49469846bcd2888200bdecfe109ec421dab9be2d\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=facf002b274bc4a18eeca03ce3a0e3ae0e7f8a1927aefacb40454ea740523ff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9d9b4144e91426d951564806ffc6dfe73b9058dbdf9d54b245e24f762fc85e88\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=7bcc3220cea9095f23823a679deafadf64a7689c7d113e934c42fcfd47732737\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=81a9bfa65af3ba069ef507c37a5372c72f74bfcaf85bcc1f68fff868e4e06083\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=6ae8e712be9f02f3ada7e45d98a18d82e9ba316a4c49c703d6d888c295f3ad42\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=a64d08de18b437b002b24734f62bbb3aa098327b90ccd2ba022ff25ee90b7b2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=ec38040df31f2fdcb1cacd0bea2211c1a83d51e341f983b8159045a1e3fc488c\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=7c52c24225589e9b905cfa1e7cae67f81ed37a4ae92debb1f4dfa914f0e7898c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built ktrain keras-bert langdetect seqeval syntok keras-transformer sacremoses keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, seqeval, sacremoses, sentencepiece, tokenizers, transformers, syntok, whoosh, ktrain\n",
            "Successfully installed cchardet-2.1.7 keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0 ktrain-0.25.0 langdetect-1.0.8 sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.19 syntok-1.3.1 tokenizers-0.9.3 transformers-3.5.1 whoosh-2.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lwaGyJb4H2j"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import ktrain\n",
        "from ktrain import text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XuM5lg34m7t",
        "outputId": "3fd5f448-d21c-40f5-d5c7-bdb11f1b8185"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voX4Vhze4nDj"
      },
      "source": [
        "#Data uploading from DRIVE\n",
        "test = pd.read_csv('/content/drive/MyDrive/BDproject/test.txt',sep=\";\",header=None, names=['sentence','label'])\n",
        "train = pd.read_csv('/content/drive/MyDrive/BDproject/train.txt',sep=\";\",header=None, names=['sentence','label'])\n",
        "val = pd.read_csv('/content/drive/MyDrive/BDproject/val.txt',sep=\";\",header=None, names=['sentence','label'])\n",
        "\n",
        "MAX_LEN = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd4GE0nY6K8R",
        "outputId": "773c734c-1ce0-4fdd-830d-8035e94180e0"
      },
      "source": [
        "# Splitting X and Y\n",
        "x_train = train.iloc[:,0] \n",
        "y_train = train.iloc[:,1] \n",
        "\n",
        "x_test = test.iloc[:,0] \n",
        "y_test = test.iloc[:,1] \n",
        "\n",
        "x_val = test.iloc[:,0] \n",
        "y_val = test.iloc[:,1] \n",
        "\n",
        "x_train,y_train = np.array(x_train),np.array(y_train)\n",
        "x_test,y_test = np.array(x_test),np.array(y_test)\n",
        "x_val,y_val = np.array(x_val),np.array(y_val)\n",
        "\n",
        "print(x_train.shape,\"|\",y_train.shape)\n",
        "print(x_test.shape,\"|\",y_test.shape)\n",
        "print(x_val.shape,\"|\",y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16000,) | (16000,)\n",
            "(2000,) | (2000,)\n",
            "(2000,) | (2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g05GEiv9cwp",
        "outputId": "3b5556ce-ee83-40af-a06d-173ff56133bb"
      },
      "source": [
        "print(train.iloc[:,1].value_counts())\n",
        "\n",
        "int_y_train = []\n",
        "int_y_test = []\n",
        "int_y_val = []\n",
        "\n",
        "for l in y_train:\n",
        "    \n",
        "    if l == \"joy\":        \n",
        "        int_y_train.append(0)\n",
        "        \n",
        "    if l == \"sadness\":       \n",
        "        int_y_train.append(1)            \n",
        "    if l == \"anger\":      \n",
        "        int_y_train.append(2)      \n",
        "    if l == \"fear\":\n",
        "        int_y_train.append(3)\n",
        "    if l == \"love\":\n",
        "        int_y_train.append(4)\n",
        "    if l == \"surprise\":\n",
        "        int_y_train.append(5)\n",
        "        \n",
        "        \n",
        "for l in y_test:\n",
        "    \n",
        "    if l == \"joy\":        \n",
        "        int_y_test.append(0)\n",
        "        \n",
        "    if l == \"sadness\":       \n",
        "        int_y_test.append(1)            \n",
        "    if l == \"anger\":      \n",
        "        int_y_test.append(2)      \n",
        "    if l == \"fear\":\n",
        "        int_y_test.append(3)\n",
        "    if l == \"love\":\n",
        "        int_y_test.append(4)\n",
        "    if l == \"surprise\":\n",
        "        int_y_test.append(5)\n",
        "        \n",
        "for l in y_val:\n",
        "    \n",
        "    if l == \"joy\":        \n",
        "        int_y_val.append(0)\n",
        "        \n",
        "    if l == \"sadness\":       \n",
        "        int_y_val.append(1)            \n",
        "    if l == \"anger\":      \n",
        "        int_y_val.append(2)      \n",
        "    if l == \"fear\":\n",
        "        int_y_val.append(3)\n",
        "    if l == \"love\":\n",
        "        int_y_val.append(4)\n",
        "    if l == \"surprise\":\n",
        "        int_y_val.append(5)\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "int_y_train,int_y_test,int_y_val = np.array(int_y_train),np.array(int_y_test),np.array(int_y_val)\n",
        "from sklearn import preprocessing\n",
        "from keras.utils import np_utils\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(int_y_train)\n",
        "\n",
        "encoded_y_train = le.transform(int_y_train)\n",
        "encoded_y_test = le.transform(int_y_test)\n",
        "encoded_y_val = le.transform(int_y_val)\n",
        "\n",
        "encoded_y_train = np_utils.to_categorical(encoded_y_train)\n",
        "encoded_y_test = np_utils.to_categorical(encoded_y_test)\n",
        "encoded_y_val = np_utils.to_categorical(encoded_y_val)\n",
        "\n",
        "print(encoded_y_train)\n",
        "\n",
        "print(int_y_train[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "joy         5362\n",
            "sadness     4666\n",
            "anger       2159\n",
            "fear        1937\n",
            "love        1304\n",
            "surprise     572\n",
            "Name: label, dtype: int64\n",
            "[[0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n",
            "[1 1 2 4 2 1 5 3 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "07cuoEOC9olh",
        "outputId": "be5005a0-dc0a-435f-a71e-4fb71487572b"
      },
      "source": [
        "class_names = ['joy', 'sadness', 'anger', 'fear', 'love', 'surprise' ]\n",
        "\n",
        "x_train_bert = x_train\n",
        "y_train_bert = encoded_y_train\n",
        "x_test_bert = x_test\n",
        "(x_train_bert,  y_train_bert), (x_test_bert, y_test_bert), preproc = text.texts_from_array(x_train=x_train_bert, y_train=y_train_bert,\n",
        "\n",
        "x_test=x_test, y_test=encoded_y_test,\n",
        "class_names=class_names,\n",
        "preprocess_mode='bert',\n",
        "maxlen=MAX_LEN, \n",
        "max_features=35000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
            "[██████████████████████████████████████████████████]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "task: text classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnDvEewl_zpI",
        "outputId": "35dcceb0-7fab-4cc2-ed1a-b0c4ab8a009a"
      },
      "source": [
        "#BERT Training and validation\n",
        "\n",
        "model_bert = text.text_classifier('bert', train_data=(x_train_bert, y_train_bert), preproc=preproc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 350\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLzCvnoEE3lI"
      },
      "source": [
        "learner = ktrain.get_learner(model_bert, train_data=(x_train_bert, y_train_bert), \n",
        "                             val_data=(x_test_bert, encoded_y_test),\n",
        "                             batch_size=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L5O-7UyFNVH",
        "outputId": "5f010923-1854-4221-9f84-3b3e114a7300"
      },
      "source": [
        "learner.fit_onecycle(2e-5, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Epoch 1/3\n",
            "2667/2667 [==============================] - 2015s 756ms/step - loss: 0.6770 - accuracy: 0.7526 - val_loss: 0.2159 - val_accuracy: 0.9200\n",
            "Epoch 2/3\n",
            "2667/2667 [==============================] - 2009s 753ms/step - loss: 0.1663 - accuracy: 0.9323 - val_loss: 0.1514 - val_accuracy: 0.9300\n",
            "Epoch 3/3\n",
            "2667/2667 [==============================] - 2003s 751ms/step - loss: 0.0900 - accuracy: 0.9519 - val_loss: 0.1360 - val_accuracy: 0.9370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e5043d438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZUYYFtCiK93",
        "outputId": "9b2cfe84-cca4-4f6c-b121-341005e171ca"
      },
      "source": [
        "#validation of BERT\n",
        "learner.validate(val_data=(x_test_bert, encoded_y_test), class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         joy       0.97      0.94      0.96       695\n",
            "     sadness       0.97      0.98      0.97       581\n",
            "       anger       0.95      0.90      0.93       275\n",
            "        fear       0.92      0.88      0.90       224\n",
            "        love       0.81      0.93      0.87       159\n",
            "    surprise       0.72      0.89      0.80        66\n",
            "\n",
            "    accuracy                           0.94      2000\n",
            "   macro avg       0.89      0.92      0.90      2000\n",
            "weighted avg       0.94      0.94      0.94      2000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[654,   1,   2,   0,  35,   3],\n",
              "       [  3, 569,   5,   4,   0,   0],\n",
              "       [  3,  14, 248,  10,   0,   0],\n",
              "       [  0,   3,   5, 196,   0,  20],\n",
              "       [ 11,   0,   0,   0, 148,   0],\n",
              "       [  3,   1,   0,   3,   0,  59]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQFQKQEUikbm",
        "outputId": "a764fda7-3b69-4664-aaf0-fec3f0c140a2"
      },
      "source": [
        "#Predicting with BERT\n",
        "predictor = ktrain.get_predictor(learner.model, preproc)\n",
        "predictor.get_classes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['joy', 'sadness', 'anger', 'fear', 'love', 'surprise']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MCTdycHYO2f"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l6tOHcEUCYN"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "135ram3IYPuo",
        "outputId": "f5decbf6-0729-4347-b53c-dd433fabf0cb"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "OqHSG5r1XHLe",
        "outputId": "62833749-b4b2-4f25-d984-7f29b436a564"
      },
      "source": [
        "text = 'I am very happy. I just broke up with my girlfriend. He is cute. I love you. I hate you. Lets go get drink'\n",
        "a_list = nltk.tokenize.sent_tokenize(text)\n",
        "n=len(a_list)\n",
        "a=0\n",
        "b=0\n",
        "c=0\n",
        "d=0\n",
        "e=0\n",
        "f=0\n",
        "for i in range (0,n):\n",
        "\n",
        "  start_time = time.time() \n",
        "  prediction = predictor.predict(a_list[i])\n",
        "  if prediction=='joy':\n",
        "    a=a+1\n",
        "  if prediction=='sadness':\n",
        "    b=b+1\n",
        "  if prediction=='anger':\n",
        "    c=c+1\n",
        "  if prediction=='fear':\n",
        "    d=d+1 \n",
        "  if prediction=='love':\n",
        "    e=e+1 \n",
        "  if prediction=='surprise':\n",
        "    f=f+1\n",
        "  print(prediction)\n",
        "t = (a+b+c+d+e+f)\n",
        "proba = {'anger': c, 'joy': a, 'sadness':b, 'love':e, 'surprise':f, 'fear':d}             \n",
        "emotion = max(proba, key=proba.get)  \n",
        "z = max(a,b,c,d,e,f)  \n",
        "percentage = z/t*100\n",
        "print('predicted emotion of the text is: ' + emotion + ' with ' + \"{:.2f}\".format(percentage) + '%')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "joy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "sadness\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "joy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "love\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "anger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "joy\n",
            "predicted emotion of the text is: joy with 50.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlZtSf-3EMQM",
        "outputId": "dc3a3b56-63fa-49d3-b280-66164460923c"
      },
      "source": [
        "#Sequential model of RNN\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "x_train_cl = []\n",
        "x_test_cl = []\n",
        "x_val_cl = []\n",
        "\n",
        "\n",
        "# Deleting stopwords\n",
        "for text in x_train:\n",
        "    \n",
        "    text = text.split()\n",
        "    text = [word for word in text if word not in stopwords]\n",
        "    text = \" \".join(text)\n",
        "    x_train_cl.append(text)\n",
        "    \n",
        "for text in x_test:\n",
        "    \n",
        "    text = text.split()\n",
        "    text = [word for word in text if word not in stopwords]\n",
        "    text = \" \".join(text)\n",
        "    x_test_cl.append(text)\n",
        "\n",
        "for text in x_val:\n",
        "    \n",
        "    text = text.split()\n",
        "    text = [word for word in text if word not in stopwords]\n",
        "    text = \" \".join(text)\n",
        "    x_val_cl.append(text)\n",
        "    \n",
        "x_train,x_test,x_val = np.array(x_train_cl),np.array(x_test_cl),np.array(x_val_cl)\n",
        "\n",
        "# We use total_text for fitting tokenizer in general \n",
        "total_text = np.concatenate((x_train,x_test,x_val),axis=0)\n",
        "\n",
        "num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words = num_words) \n",
        "\n",
        "tokenizer.fit_on_texts(total_text)\n",
        "\n",
        "list(tokenizer.word_index)[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feel',\n",
              " 'feeling',\n",
              " 'like',\n",
              " 'im',\n",
              " 'really',\n",
              " 'know',\n",
              " 'time',\n",
              " 'get',\n",
              " 'little',\n",
              " 'people']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEaNd3yqEUTr"
      },
      "source": [
        "# Tokenizing everything\n",
        "\n",
        "x_train_token = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_token = tokenizer.texts_to_sequences(x_test)\n",
        "x_val_token = tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "total_token = np.concatenate((x_train_token,x_test_token,x_val_token),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiAF5IPZEvNq",
        "outputId": "6704c3c9-6e12-4095-d148-603e96e1a73f"
      },
      "source": [
        "# Padding\n",
        "print(np.mean([len(text) for text in total_token]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.6699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWh49g9lExHO",
        "outputId": "6f1c858b-3b5d-49ed-e702-7f39d5e9441c"
      },
      "source": [
        "x_train_pad = pad_sequences(x_train_token,20) \n",
        "x_test_pad = pad_sequences(x_test_token,20)\n",
        "x_val_pad = pad_sequences(x_val_token,20)\n",
        "\n",
        "print(x_train_pad[0],end=\"\\n-------------------------------------------------------------------------\\n\")\n",
        "print(x_train_pad[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  50\n",
            "   1 495]\n",
            "-------------------------------------------------------------------------\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0   30    2\n",
            "  464  425   44   54 1573 1304]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Hhg4tHEys-",
        "outputId": "40b5d4e0-06ec-4a46-b18b-c4f449ce1194"
      },
      "source": [
        "#Sequential Modeling and Predicting\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense,CuDNNGRU,Embedding,Bidirectional\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=2000\n",
        "                   ,output_dim=100\n",
        "                   ,input_length=20))\n",
        "\n",
        "model.add(Bidirectional(CuDNNGRU(units=16,return_sequences=True)))\n",
        "\n",
        "model.add(Bidirectional(CuDNNGRU(units=8)))\n",
        "\n",
        "model.add(Dense(6,activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 100)           200000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 20, 32)            11328     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 16)                2016      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 213,446\n",
            "Trainable params: 213,446\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD7PT8pgFY2w",
        "outputId": "1bc42b70-5e3c-43f1-d57c-d7f884d9412c"
      },
      "source": [
        "print(x_train_pad.shape)\n",
        "print(y_train.shape)\n",
        "model.fit(x_train_pad,encoded_y_train,epochs=10,batch_size=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16000, 20)\n",
            "(16000,)\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 1.1830 - accuracy: 0.5272\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.4375 - accuracy: 0.8611\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.2453 - accuracy: 0.9186\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.1771 - accuracy: 0.9334\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.1453 - accuracy: 0.9415\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.1288 - accuracy: 0.9488\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.1173 - accuracy: 0.9524\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.1079 - accuracy: 0.9557\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.0985 - accuracy: 0.9603\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 6s 8ms/step - loss: 0.0922 - accuracy: 0.9626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c01b21f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR6xmNzlkkBS",
        "outputId": "4678cd37-7b91-4c72-86a8-31e75f8a3e40"
      },
      "source": [
        "#Predicting with the sequential RNN\n",
        "preds = model.predict_classes(x_test_pad)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(preds,int_y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9245"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRtPtQsImenA",
        "outputId": "2ccb354a-52db-491d-ded9-b0a3a817b88b"
      },
      "source": [
        "message = 'I love you'\n",
        "message_sequential = message\n",
        "message_sequential_token = tokenizer.texts_to_sequences(message_sequential)\n",
        "message_sequential_pad = pad_sequences(message_sequential_token,20) \n",
        "\n",
        "\n",
        "flat_list = []\n",
        "for sublist in message_sequential_pad:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "\n",
        "flat_list = [flat_list]\n",
        "\n",
        "message_sequential = pad_sequences(flat_list, padding='post', maxlen=MAX_LEN)\n",
        "\n",
        "model.predict(message_sequential)*100\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.033682  , 14.613419  , 57.593803  , 24.412458  ,  0.19847986,\n",
              "         0.14815678]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OatvMdEtkrmB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "9e518b6c-7917-46c5-d907-6e960df8db62"
      },
      "source": [
        "#Final Model which combines two above trained algorithms\n",
        "text = 'i never make her separate from me because i don t ever want her to feel like i m ashamed with her. i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived. i cant walk into a shop anywhere where i do not feel uncomfortable. i felt anger when at the end of a telephone call. i like to have the same breathless feeling as a reader eager to see what will happen next'\n",
        "a_list = nltk.tokenize.sent_tokenize(text)\n",
        "n=len(a_list)\n",
        "a=0\n",
        "b=0\n",
        "c=0\n",
        "d=0\n",
        "e=0\n",
        "f=0\n",
        "a_rnn=0\n",
        "b_rnn=0\n",
        "c_rnn=0\n",
        "d_rnn=0\n",
        "e_rnn=0\n",
        "f_rnn=0\n",
        "\n",
        "for i in range (0,n):\n",
        "\n",
        "  start_time = time.time() \n",
        "  words_list = word_tokenize(a_list[i])\n",
        "  \n",
        "  words_quantity = len(words_list)\n",
        "\n",
        "  text_sequential_token = tokenizer.texts_to_sequences(a_list[i])\n",
        "  text_sequential_pad = pad_sequences(text_sequential_token,20)\n",
        "\n",
        "  flat_list = []\n",
        "  for sublist in text_sequential_pad:\n",
        "      for item in sublist:\n",
        "          flat_list.append(item)\n",
        "\n",
        "  flat_list = [flat_list]\n",
        "\n",
        "  text_sequential = pad_sequences(flat_list, padding='post', maxlen=MAX_LEN)\n",
        "  \n",
        "  prediction_rnn=model.predict(text_sequential_pad)\n",
        "  prediction = predictor.predict(a_list[i])\n",
        "  emotion_rnn = y_val[prediction_rnn.argmax()]\n",
        "  \n",
        "  if prediction=='joy':\n",
        "    a=(a+1)*words_quantity\n",
        "  if prediction=='sadness':\n",
        "    b=(b+1)*words_quantity\n",
        "  if prediction=='anger':\n",
        "    c=(c+1)*words_quantity\n",
        "  if prediction=='fear':\n",
        "    d=(d+1)*words_quantity \n",
        "  if prediction=='love':\n",
        "    e=(e+1)*words_quantity\n",
        "  if prediction=='surprise':\n",
        "    f=(f+1)*words_quantity\n",
        "\n",
        "  if emotion_rnn=='joy':\n",
        "    a_rnn=(a_rnn+1)*words_quantity\n",
        "  if emotion_rnn=='sadness':\n",
        "    b_rnn=(b_rnn+1)*words_quantity\n",
        "  if emotion_rnn=='anger':\n",
        "    c_rnn=(c_rnn+1)*words_quantity\n",
        "  if emotion_rnn=='fear':\n",
        "    d_rnn=(d_rnn+1)*words_quantity\n",
        "  if emotion_rnn=='love':\n",
        "    e_rnn=(e_rnn+1)*words_quantity \n",
        "  if emotion_rnn=='surprise':\n",
        "    f_rnn=(f_rnn+1)*words_quantity\n",
        "\n",
        "  print('--------------------------------------------------------------------------')\n",
        "  print('Sentence: {} \\nPredicted_bert: {}'.format(a_list[i], prediction))\n",
        "  print('Predicted_rnn: {}'.format(emotion_rnn))\n",
        "  print('Word number:',words_quantity)\n",
        "\n",
        "t = (a+b+c+d+e+f)\n",
        "t_rnn = (a_rnn+b_rnn+c_rnn+d_rnn+e_rnn+f_rnn)\n",
        "total = t+t_rnn\n",
        "\n",
        "proba = {'anger': c, 'joy': a, 'sadness':b, 'love':e, 'surprise':f, 'fear':d}             \n",
        "proba_rnn = {'anger': c_rnn, 'joy': a_rnn, 'sadness':b_rnn, 'love':e_rnn, 'surprise':f_rnn, 'fear':d_rnn}             \n",
        "emotion_bert = max(proba, key=proba.get)\n",
        "emotion_rnn_1 = max(proba_rnn, key=proba_rnn.get)\n",
        "\n",
        "numbers = [a,b,c,d,e,f]\n",
        "numbers_rnn = [a_rnn,b_rnn,c_rnn,d_rnn,e_rnn,f_rnn]\n",
        "z = max(numbers)\n",
        "z_rnn = max(numbers_rnn)\n",
        "percentage_bert = z/total*100\n",
        "percentage_rnn = z_rnn/total*100\n",
        "\n",
        "\n",
        "if percentage_bert >= percentage_rnn:\n",
        "  emotion = emotion_bert\n",
        "  percentage = percentage_bert\n",
        "  new1 = proba_rnn.get(emotion_bert)\n",
        "  numbers_rnn.remove(new1)\n",
        "  second_largest_integer_rnn = max(numbers_rnn)\n",
        "  numbers.remove(z)\n",
        "  second_largest_integer = max(numbers)\n",
        "  proba.pop(emotion_bert)\n",
        "  proba_rnn.pop(emotion_bert)\n",
        "  \n",
        "else: \n",
        "  emotion = emotion_rnn_1\n",
        "  percentage = percentage_rnn\n",
        "  new1 = proba.get(emotion_rnn_1)\n",
        "  numbers_rnn.remove(z_rnn)\n",
        "  second_largest_integer_rnn = max(numbers_rnn)\n",
        "  numbers.remove(new1)\n",
        "  second_largest_integer = max(numbers)\n",
        "  proba.pop(emotion_rnn_1)\n",
        "  proba_rnn.pop(emotion_rnn_1)\n",
        "\n",
        "emotion_bert_2 = max(proba, key=proba.get)\n",
        "emotion_rnn_2 = max(proba_rnn, key=proba_rnn.get)\n",
        "percentage_bert_2 = second_largest_integer/total*100\n",
        "percentage_rnn_2 = second_largest_integer_rnn/total*100\n",
        "\n",
        "if percentage_bert_2 > percentage_rnn_2:\n",
        "  emotion2 = emotion_bert_2\n",
        "  percentage2 = percentage_bert_2\n",
        "else: \n",
        "  emotion2 = emotion_rnn_2\n",
        "  percentage2 = percentage_rnn_2\n",
        "print('--------------------------------------------------------------------------')\n",
        "print('Predicted emotion of the full text is: ' + emotion + ' with ' + \"{:.2f}\".format(percentage) + '% '+'and ' + emotion2 + ' with ' + \"{:.2f}\".format(percentage2)+'%')\n",
        "print('--------------------------------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------\n",
            "Sentence: i never make her separate from me because i don t ever want her to feel like i m ashamed with her. \n",
            "Predicted_bert: sadness\n",
            "Predicted_rnn: love\n",
            "Word number: 23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------\n",
            "Sentence: i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived. \n",
            "Predicted_bert: joy\n",
            "Predicted_rnn: anger\n",
            "Word number: 22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------\n",
            "Sentence: i cant walk into a shop anywhere where i do not feel uncomfortable. \n",
            "Predicted_bert: fear\n",
            "Predicted_rnn: fear\n",
            "Word number: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------\n",
            "Sentence: i felt anger when at the end of a telephone call. \n",
            "Predicted_bert: anger\n",
            "Predicted_rnn: joy\n",
            "Word number: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------\n",
            "Sentence: i like to have the same breathless feeling as a reader eager to see what will happen next \n",
            "Predicted_bert: joy\n",
            "Predicted_rnn: joy\n",
            "Word number: 18\n",
            "--------------------------------------------------------------------------\n",
            "Predicted emotion of the full text is: joy with 54.76% and love with 3.04%\n",
            "--------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-QLTByNTaA-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}